{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorFlow in SageMaker - Quickstart\n",
    "\n",
    "Starting by the TensorFlow's framework version 1.11, you can use the SageMaker TensorFlow Container to train any TensorFlow script. \n",
    "\n",
    "For this example, you use [Multi-layer Recurrent Neural Networks (LSTM, RNN) for character-level language models in Python using Tensorflow](https://github.com/sherjilozair/char-rnn-tensorflow). You can use the same technique for other scripts or repositories. For example, [TensorFlow Model Zoo](https://github.com/tensorflow/models) and [TensorFlow benchmark scripts](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "For training data, use plain text versions of Sherlock Holmes stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = os.path.join(os.getcwd(), 'sherlock')\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "!wget https://sherlock-holm.es/stories/plain-text/cnus.txt --force-directories --output-document=sherlock/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the training script\n",
    "\n",
    "Let's start by cloning the repository that contains the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sherjilozair/char-rnn-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository includes a [README.md](https://github.com/sherjilozair/char-rnn-tensorflow/blob/master/README.md#basic-usage) with an overview of the project, requirements, and basic usage:\n",
    "\n",
    "> #### **Basic Usage**\n",
    "> _To train with default parameters on the tinyshakespeare corpus, run **python train.py**. \n",
    "To access all the parameters use **python train.py --help.**_\n",
    "\n",
    "[train.py](https://github.com/sherjilozair/char-rnn-tensorflow/blob/master/train.py#L11) uses the [argparse](https://docs.python.org/3/library/argparse.html) library and requires the following arguments:\n",
    "\n",
    "```python\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# Data and model checkpoints directories\n",
    "parser.add_argument('--data_dir', type=str, default='data/tinyshakespeare', help='data directory containing input.txt with training examples')\n",
    "parser.add_argument('--save_dir', type=str, default='save', help='directory to store checkpointed models')\n",
    "...\n",
    "args = parser.parse_args()\n",
    "\n",
    "```\n",
    "When SageMaker training finishes, it deletes all data generated inside the container with exception of the directories **_/opt/ml/model_** and **_/opt/ml/output_**. To ensure that model data is not lost during training, training scripts are invoked in SageMaker with an additional argument **--model_dir**, that needs to be handle by the training script. \n",
    "\n",
    "We need to replace the argument **--save_dir** with the required argument **--model_dir**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this command will replace data_dir by model_dir in the training script\n",
    "!sed -i 's/save_dir/model_dir/g' char-rnn-tensorflow/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the training script can executed in the container as shown bellow:\n",
    "\n",
    "> ```bash\n",
    "python train.py --num-epochs 1 --data_dir /opt/ml/input/data/training --model_dir /opt/ml/model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test locally using SageMaker Python SDK TensorFlow Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the SageMaker Python SDK [TensorFlow](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/README.rst#training-with-tensorflow) estimator to easily train locally and in SageMaker. To train locally, you can set the instance type to [local](https://github.com/aws/sagemaker-python-sdk#local-mode) as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# sets the script arguments --num_epochs and --data_dir\n",
    "hyperparameters = {'num_epochs': 1, \n",
    "                   'data_dir': '/opt/ml/input/data/training'}\n",
    "\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='char-rnn-tensorflow',\n",
    "                       train_instance_type='local',      # Run in local mode\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(), # Passes to the container the AWS role that you are using on this notebook\n",
    "                       framework_version='1.11.0', # Uses TensorFlow 1.11\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)\n",
    "             \n",
    "\n",
    "estimator.fit({'training': f'file://{data_dir}'}) # Starts training and creates a data channel named training with the contents\n",
    "# data_dir in the folder /opt/ml/input/data/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Script Mode executes the script in the container\n",
    "\n",
    "The above cell downloads SageMaker TensorFlow container with TensorFlow Python 3, CPU version, locally and simulates a SageMaker training job. \n",
    "When training starts, SageMaker TensorFlow executes **train.py**, passing **hyperparameters** and **model_dir** as script arguments. The example above is executed as follows:\n",
    "```bash\n",
    "python -m train --num-epochs 1 --data_dir /opt/ml/input/data/training --model_dir /opt/ml/model\n",
    "```\n",
    "\n",
    "Let's explain the values of **--data_dir** and **--model_dir** with more details:\n",
    "\n",
    "- **/opt/ml/input/data/training** is the directory inside the container where the training data is downloaded. The data is downloaded to this folder because **training** is the channel name defined in ```estimator.fit({'training': 'file://{data_dir'}})```. See [training data](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container-trainingdata) for more information. \n",
    "\n",
    "- **/opt/ml/model** use this directory to save models, checkpoints, or any other data. Any data saved in this folder is saved in the S3 bucket defined for training. See [model data](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-envvariables) for more information.\n",
    "\n",
    "### Reading additional information from the container\n",
    "\n",
    "Often, a user script needs additional information from the container that is not available in ```hyperparameters```.\n",
    "SageMaker containers write this information as **environment variables** that are available inside the script.\n",
    "\n",
    "For example, the example above can read information about the **training** channel provided in the training job request by adding the environment variable `SM_CHANNEL_TRAINING` as the default value for the `--data_dir` argument:\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # reads input channels training and testing from the environment variables\n",
    "  parser.add_argument('--data_dir', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n",
    "```\n",
    "\n",
    "Script mode displays the list of available environment variables in the training logs. You can find the [entire list here](https://github.com/aws/sagemaker-containers/blob/master/README.md#environment-variables-full-specification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you test the training job locally, upload the dataset to an S3 bucket so SageMaker can access the data during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "training_data = sagemaker.Session().upload_data(path='sherlock', key_prefix='datasets/sherlock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned variable inputs above is a string with a s3 location which SageMaker Tranining has permissions\n",
    "to read data from. **It has education purposes, requiring\n",
    " more robust solutions for larger datasets:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train in SageMaker:\n",
    "- change the estimator argument **train_instance_type** to any SageMaker ml instance available for training.\n",
    "- set the **training** channel to a S3 location.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='char-rnn-tensorflow',\n",
    "                       train_instance_type='ml.c4.xlarge', # Executes training in a ml.c4.xlarge instance\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       framework_version='1.11.0',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)\n",
    "             \n",
    "\n",
    "estimator.fit({'training': f'file://{data_dir}'}) # Starts training and creates a data channel named \n",
    "# training with the contents of data dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using script launchers for training\n",
    "In our previous example, we edited **train.py** to train in SageMaker. Another option is to provide a launcher script as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile char-rnn-tensorflow/launcher.sh # this line creates a file named launcher.sh under char-rnn-tensorflow\n",
    "\n",
    "python train.py --data_dir /opt/ml/data/training --save_dir /opt/ml/model --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a **`TensorFlow`** estimator using **laucher.sh** as the entry-point. Notice that the hyperparameters are not necessary anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='launcher.sh',\n",
    "                       source_dir='char-rnn-tensorflow',\n",
    "                       train_instance_type='local',\n",
    "                       train_instance_count=1,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       framework_version='1.11.0',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)\n",
    "             \n",
    "\n",
    "estimator.fit({'training': training_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laucher scripts are executed with the same arguments of Python scripts and have access to the same environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile launcher.sh \n",
    "\n",
    "# prints SageMaker environment variables before training\n",
    "printenv | grep SM_\n",
    "\n",
    "# invokes the training script using values from environment variables\n",
    "python train.py --data_dir ${SM_TRAINING_CHANNEL} --save_dir ${SM_MODEL} --epochs ${SM_HP_EPOCHS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's detail the environment variables used above:\n",
    "\n",
    "- **`SM_MODEL`**: the directory inside the container where the training model data must be saved inside the container, i.e. **/opt/ml/model**.\n",
    "- **`SM_TRAINING_CHANNEL`**: the directory inside the container where training data from the channel **training** is available, i.e. **/opt/ml/model**.\n",
    "- **`SM_HP_EPOCHS`**: value of the hyperparameter named **epochs**. This hyperparameter needs to be provide by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laucher scripts can be handy for multiple scenarios including:\n",
    "\n",
    "## Installing requirements before training\n",
    "\n",
    "\n",
    "```bash\n",
    "> launcher.sh\n",
    "\n",
    "apt-get install cowsay -y\n",
    "pip install tensorflow==1.12\n",
    "```\n",
    "\n",
    "```python\n",
    "TensorFlow(entry_point='launcher.sh', ...).fit()\n",
    "```\n",
    "\n",
    "## Training using different computer languages\n",
    "The example below install **TensorFlow for C** and executes a hello world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tf_c/hello_tf.c\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <tensorflow/c/c_api.h>\n",
    "\n",
    "int main() {\n",
    "\n",
    "    printf(\"Hello from TensorFlow C library version %s\", TF_Version());\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tf_c/launcher.sh\n",
    "\n",
    "wget -q -t 3 https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.11.0.tar.gz\n",
    "    \n",
    "tar -xzvf libtensorflow-cpu-linux-x86_64-1.11.0.tar.gz -C /usr/local\n",
    "\n",
    "ldconfig\n",
    "\n",
    "gcc -I/usr/local/include -L/usr/local/lib hello_tf.c -ltensorflow -o hello_tf\n",
    "\n",
    "./hello_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_c_estimator = TensorFlow(entry_point='launcher.sh',\n",
    "                            source_dir='tf_c',\n",
    "                            train_instance_type='local',\n",
    "                            train_instance_count=1,\n",
    "                            role=sagemaker.get_execution_role(),\n",
    "                            framework_version='1.11.0',\n",
    "                            py_version='py3',\n",
    "                            script_mode=True)\n",
    "             \n",
    "\n",
    "tf_c_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acessing training output information\n",
    "**`estimator.output_path`** contains the S3 location where training outputs are stored after training. The training output includes data from the special directories **/opt/ml/output** and **/opt/ml/model**.\n",
    "\n",
    "### Acessing training model data\n",
    "Model data written to **/opt/ml/model** during training is compressed in a tar file named **model.tar.gz** and uploaded to S3. **`estimator.model_data()`** returns the location of **model.tar.gz**.\n",
    "\n",
    "### Running multiple training jobs in parallel\n",
    "```estimator.fit``` waits for the training job to end by default, and streams the training logs. You can run training jobs asyncronouly be setting the attribute ```wait``` to false:\n",
    "\n",
    "```python\n",
    "estimator.fit(inputs={'training': training_data}, wait=False)\n",
    "```\n",
    "\n",
    "### Retrieving training job information\n",
    "You can use the ```estimator.attach(training_job_name)``` to retrieve information of a running training job:\n",
    "\n",
    "```python\n",
    "estimator = TensorFlow.attach('my-training-job')\n",
    "```\n",
    "You can use the aws cli to access training job information from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker describe-training-job --training-job-name {estimator.latest_training_job.job_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the [aws console page](https://console.aws.amazon.com/sagemaker/home?#/jobs) to access information about training jobs as well.\n",
    "\n",
    "You can use the boto library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker_client = boto3.Session().client('sagemaker')\n",
    "\n",
    "sagemaker_client.describe_training_job(TrainingJobName=estimator.latest_training_job.job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS sagemaker provides libraries most common languages including C, C#, Java, Javascript, and others. **add links**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using boto3 and the command line\n",
    "Some other useful actions acessible from boto3 and awscli include:\n",
    "\n",
    "#### Stopping a training job\n",
    "**aws cli**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker stop-training-job--training-job-name {estimator.latest_training_job.job_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**boto3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.stop_training_job(TrainingJobName=estimator.latest_training_job.job_name) # double check this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**aws cli**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker list-training-jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**boto 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.list_training_jobs() # double check this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access awscli sagemaker for a complete list of commands or type **`aws sagemaker help`** in the command line.\n",
    "\n",
    "You can access sagemaker boto3 for the complete documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the command line for training with SageMaker Tensorflow container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# Usage:\n",
    "#   train.sh <TRAINING_IMAGE> <JOB_NAME> <ROLE_ARN> <TRAINING_DATA> <DEFAULT_BUCKET> <SOURCE_DIR> <ENTRY_POINT>\n",
    "#\n",
    "\n",
    "TRAINING_IMAGE=$1\n",
    "JOB_NAME=$2\n",
    "ROLE_ARN=$3\n",
    "TRAINING_DATA=$4\n",
    "IMAGE_NAME=$5\n",
    "SOURCE_DIR=$6\n",
    "ENTRY_POINT=$7\n",
    "\n",
    "tar -cvzf source.tar.gz $SOURCE_DIR{}\n",
    "fiowaengfioewahjgioewhio\n",
    "\n",
    "INPUTS = \"ChannelName = training,\\\n",
    "          DataSource = {\\\n",
    "              S3DataSource = {\\\n",
    "                  S3DataType = S3Prefix,\\\n",
    "                  S3Uri = $TRAINING_DATA,\\\n",
    "                  S3DataDistributionType = FullyReplicated\\\n",
    "              },\\\n",
    "          },\\\n",
    "          ContentType = string,\\\n",
    "          CompressionType = None,\\\n",
    "          RecordWrapperType = None,\\\n",
    "          InputMode = File\"\n",
    "\n",
    "ALGO_SPEC = \"TrainingImage = ${TRAINING_IMAGE}, TrainingInputMode = File\"\n",
    "\n",
    "\n",
    "aws sagemaker create-training-job \\\n",
    "--training-job-name ${JOB_NAME} \\\n",
    "--role-arn ${ROLE_ARN} \\\n",
    "--input-data-config ${INPUTS} \\\n",
    "--output-data-config \"S3OutputPath = ${DEFAULT_BUCKET}/${JOB_NAME}\" \\\n",
    "--resource-config \"InstanceType = ml.c4.xlarge, InstanceCount = 1, VolumeSizeInGB = 30\" \\\n",
    "--stopping-condition \"MaxRuntimeInSeconds = 3600\" \\\n",
    "--hyper-parameters \"sagemaker_submit_directory = ${}, sagemaker_program = ${ENTRY_POINT}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = estimator.train_image()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker.Session().default_bucket\n",
    "\n",
    "!train.sh  {training_image} tensorflow-quickstart-$(date +%s) {role} {training_data} {default_bucket}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}