{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: The computational real neuron. \n",
    "\n",
    "Welcome to the SageMaker workshop. \n",
    "\n",
    "## Supervised Learning.\n",
    "\n",
    "**Supervised learning is the task of arriving at a mathematical mapping function from the co-variate space to the variate space using a labeled training dataset.** The training dataset is of a set of co-variate - variate sample mapping. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). Colloquially, various names are used for the co-variates and variates, the most common ones being 'features' and 'lables'.\n",
    "\n",
    "Let us create a relatable and lower-dimensional dataset to study supervised learning. Assume that you are a human resource manager at Amazon and that we are planning to make strategic human resource expansions in your department. While mkaing great hiring decisions, we would like to know antecedently how much that candidate’s pay scale is likely to be. In today’s market where data scientists are in strong demand, most candidates have a free-market value they are predisposed to expect. As a data scientist yourself, and keeping with Amazon's tradition of relenetlessly relying on data, we could use machine learning to model a future candidate’s potential compensation. Using this knowledge, we can negotiate during the interview. \n",
    "\n",
    "Let us use the compensation of some of our colleagues who are already employed in the org in estimating a future employee’s pay. Say your org has $n+m$ employees who are willing to provide you their compensation data. If you create a dataset of your current employees, you may come up with something that looks like the following (considering for now only the first $n$ employees as the training data):\n",
    "$$ D = \\begin{bmatrix}\n",
    "    \\bf{x_1} & y_1 \\\\ \n",
    "    \\bf{x_2} & y_2 \\\\\n",
    "    \\vdots & \\vdots \\\\\n",
    "    \\bf{x_n} & y_n \\end{bmatrix},$$\n",
    "where, $\\bf{x_i} \\in \\mathbb{R}^d$ is a d-dimensional (vector) sample where each sample represents an existing employee and each dimesnion of this sample corresponds to an attribute of the employee that is related to their compensation and $y_i \\in \\mathbb{R}^1$ is the salary of the respective employee. \n",
    "\n",
    "In this dataset, **to *learn* is to establish a mapping between the features and the labels.** To model the compensation of the employees, consider for now that, $x_i \\in \\mathbb{R}^1$, is a one-dimensional feature, perhaps the number of years of experience a candidate has in the field. The provided code has a data simulator that will generate some syntehtic data to mimic this scenario. The data might look like something like what is generated by the code-block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import dataset_generator\n",
    "# Initialize a dataset creator\n",
    "dataset = dataset_generator(dimensions = 1)\n",
    "dataset._demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is created as follow:\n",
    "\n",
    "* Consider a real line from which the dataset is supposed to be derived from.\n",
    "* Add a small amount of noise to the data on the line.\n",
    "* Sample from this noisy line. \n",
    "\n",
    "Implementationally, the class `dataset_generator` uses the following piece of code to achieve this. The class constructors, creates a `w` and `b` parameter randomly. The class also instantiates a `mu` and `sigma` for the noise generator. Instantiating these parameters helps us query from the same data distribution over and over again. \n",
    "\n",
    "```python \n",
    "self.w = np.random.rand(self.dimensions,1)\n",
    "self.b = np.random.rand(1)\n",
    "self.mu = 0\n",
    "self.sigma = 0.5\n",
    "```\n",
    "Every time the `dataset_generator.query_method` method is callled a new `x` and `y` are sampled from this line.\n",
    "```python\n",
    "x = np.random.uniform(size = (samples, self.dimensions), low = 0, high = 10)        \n",
    "y = np.dot(x, self.w) + np.random.normal(self.mu, self.sigma, (samples,1)) + self.b\n",
    "```\n",
    "Formally, Supervised learning is the process of establishing through a model, the relationship or mapping between $(\\bf{x} ,y)$ using $(\\bf{x_i},y_i) \\in D \\forall i \\in [i,n]$, such that once established, given a new sample $x_{n+j} \\notin D$ and $j<m$, the model estimates $y_{n+j}$. Informally, we want to learn a model using the training part of the dataset, such that we should be able to predict the associated label for samples from the remaining. The dataset $D$ is called the training dataset and the set of samples $x_i, i \\in (n, n + m]$ is called the generalization dataset. The generalization dataset may also be the real world. If we have the knowledge of the actual targets of the samples in the generalization set, we call it the testing set, as we can use it to test the quality of our model before deploying it in the real world.\n",
    "\n",
    "Models are usually of the form $\\hat{y} = g(\\bf{X},w)$ where, $w$ are the parameters or weights of the model that transform $X$ to $\\hat{y}$. In short, a model is a functional form that was predetermined that depends on some to-be determined parameters whose values are to be learnt using the data.\n",
    "\n",
    "## Least Squares Linear Regression.\n",
    "\n",
    "Let us posit that the experience of the candidates and their compensation are **linearly related**. What this means is that we are making a decision that the relationship between the candidates’ experience and the salaries is captured by a straight line. With this assumption, we have are limiting the architecture of our problem to linear models and converted our problem into a linear regression problem. Essentially, if our data is $x \\in \\mathbb{R}^1$, then our prediction is, \n",
    "$$ \\hat{y} = w_1x + b.$$\n",
    "If $\\bf{x} \\in \\mathbb{R}^d $, then \n",
    "$$ \\hat{y} = \\sum_{i=1}^d w_ix^i + b.$$\n",
    "\n",
    "To know how good our predictions are we need some metric to measure our errors. Consider the root-mean-squared error or the RMSE,\n",
    "$$ e_i(\\bf{w}) = \\vert \\vert \\hat{y_i} - y_i \\vert \\vert_2, $$\n",
    "which, will tell us how *far* away our prediction $\\hat{y_i}$ is from the actual value $y_i, \\forall i \\in [0,n]$ in the Euclidean sense. For our entire dataset, we can have a cumulative error defined as,\n",
    "$$e(\\bf{w}) = \\sum_{i=1}^n \\vert \\vert y_i - \\hat{y_i} \\vert \\vert_2,$$\n",
    "or,\n",
    "$$ e(\\bf{w}) = \\sum_{i=1}^n \\vert \\vert y_i - W^TX + b \\vert \\vert_2.$$\n",
    "\n",
    "This error is often referred to as the objective. This is what we want to **minimize**. We want those parameters $w$, that will get us to be as low as possible $e(w)$. Formally, we want,\n",
    "$$ \\hat{w} = \\arg\\min_w e(w). $$\n",
    "We can derive a solution for this optimization problem analytically.\n",
    "$$ e(w) = \\frac{1}{2}(y-w^TX)^T(y-w^TX),$$\n",
    "$$\\frac{\\partial e}{\\partial w} = -X^Tt + X^TXw,$$\n",
    "equating this to zero to obtain minima we get,\n",
    "$$X^TXw = X^Ty,$$\n",
    "$$\\hat{w} = (X^TX)^{-1}X^Ty.$$\n",
    "$\\hat{w}$ is will give us the minimum most error possible and this solution is called the analytical solution.\n",
    "\n",
    "### Implementing the analytical solution.\n",
    "\n",
    "Let us run our dataset through this analytical solution and see if it will work. The regressor base class is defined in [analytical_solution](analytical_solution.py) file. The core of this code is [line 21](analytical_solution.py#L21), which is the following.\n",
    "```python \n",
    "w = np.dot(np.linalg.pinv(np.dot(x.T,x)), np.dot(x.T,self.y))\n",
    "```\n",
    "As can be seen, it is a direct implementation of the analytical solution.\n",
    "\n",
    "To see this in action let us create a training dataset of 40 samples from our generator and use our regressor to estimate the analytical $w$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytical_solution import regressor\n",
    "data_train = dataset.query_data(samples = 40) # Create a training dataset.  \n",
    "r = regressor(data_train)  # This call should return a regressor object that is fully trained.\n",
    "params = r.get_params() # This call should return parameters of the model that are fully trained.\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see how well we are doing. Let us use the same dataset distribution to query 40 additional samples for validation. We can use this validation dataset to make predictions for the linear regressor. Once we have the predictions, we can use the RMSE to check how well we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from errors import rmse\n",
    "data_test = dataset.query_data(samples = 40)  # Create a random testing dataset.\n",
    "predictions = r.get_predictions(data_test[0]) # This call should return predictions.\n",
    "print (\"Rmse error of predictions = \" + str(rmse(data_test[1], predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a line. We should be able to visualize this line as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.plot(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also solve the linear regression problem for multi-dimensional data, only we will get more parameters as output. Let us run the same piece of code for a 20 dimensional data and checkout the paramerter shapes. Notice how the shapes work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_d_dataset = dataset_generator(dimensions = 20)\n",
    "multi_d_train_data = multi_d_dataset.query_data(samples = 400)\n",
    "multi_d_r = regressor(multi_d_train_data)\n",
    "params = multi_d_r.get_params()\n",
    "print(\"Shape of x: \" + str(multi_d_train_data[0].shape))\n",
    "print(\"Shape of Weights: \" + str(params[0].shape))\n",
    "print(\"Shape of Bias\" + str(params[1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see here that our simple model works pretty fine. Although for this simple linear model an analytical solution does exist, we find that for more complex problem structures we have to rely on some optimization procedures that are described in the later lectures.\n",
    "\n",
    "## Ridge Regression.\n",
    "\n",
    "We used a ``numpy.linalg.pinv`` to solve this problem. We did this because not always is $x^Tx$ invertible. What can we do in our analytical solution to make this invertible? One thing that can be done to make this solution more stable is to ensure that the diagonal elements of $w^Tw$ behave nicely. Consider the following analytical solution for $\\hat{w}$,\n",
    "$$\\hat{w} = (X^TX + \\alpha_2I)^{-1}X^Ty.$$\n",
    "In this solution, you can be assured to a reasonable degree that we will give a good solution. What is this a solution for? \n",
    "Consider the error function,\n",
    "$$e(w)=(y-w^Tx)^T(y-w^Tx) + \\alpha_2w^Tw.$$\n",
    "Now,\n",
    "$$\\frac{\\partial e}{\\partial w} = \\frac{\\partial}{\\partial w} ( w^Tx^Txw - 2y^Txw + y^Ty + \\alpha_2w^Tw),$$\n",
    "$$ = 2x^Txw - 2x^Ty + 2\\alpha_2I,$$\n",
    "$$ = 2(x^Tx + \\alpha_2I)w - 2X^Ty,$$\n",
    "which, when equated to zero to obtain the minima we get,\n",
    "$$(x^Tx + \\alpha_2I)w = X^Ty,$$\n",
    "$$\\hat{w} = (X^TX + \\alpha_2I)^{-1}X^Ty.$$\n",
    "\n",
    "Fortunately, the hack we used to get a well-behaved solution turned out to be the solution for an error function where we penalize the function with the $L_2$ norm of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytical_solution import ridge_regressor\n",
    "data_train = dataset.query_data(samples = 40) # Create a training dataset.  \n",
    "r = ridge_regressor(data_train, alpha = 1)  # This call should return a regressor object that is fully trained.\n",
    "ridge_params = r.get_params()    # This call should return parameters of the model that are \n",
    "                                 # fully trained.\n",
    "data_test = dataset.query_data(samples = 40)  # Create a random testing dataset.\n",
    "predictions = r.get_predictions(data_test[0]) # This call should return predictions.\n",
    "print (\"Rmse error of predictions = \" + str(rmse(data_test[1], predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Geometry of L2 Regularization](figures/regularization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython import display\n",
    "for noisy_samples in range(0,100,5):\n",
    "    data_train_noisy = dataset.query_noisy_data(samples = 50, noisy_samples = noisy_samples)\n",
    "    no_reg = regressor(data_train_noisy)  \n",
    "    predictions = no_reg.get_predictions(data_test[0]) \n",
    "    print (\"Rmse error of predictions with \" + str(noisy_samples) +  \" samples  = \" + \\\n",
    "            str(rmse(data_test[1], predictions)))\n",
    "    no_reg.plot(data_train_noisy)\n",
    "    display.clear_output(wait=True)\n",
    "    sleep(.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_noisy = dataset.query_noisy_data(samples = 50, noisy_samples = 5)\n",
    "for alpha in range (0,5000,250):\n",
    "    l2_reg = ridge_regressor(data_train_noisy, alpha = alpha )\n",
    "    predictions = l2_reg.get_predictions(data_test[0]) \n",
    "    print (\"Rmse error of predictions with \" + str(alpha) +  \" alpha  = \" + \\\n",
    "            str(rmse(data_test[1], predictions)))\n",
    "    l2_reg.plot(data_train_noisy)\n",
    "    display.clear_output(wait=True)\n",
    "    sleep(.1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noisy_samples in range(0,100,5):\n",
    "    data_train_noisy = dataset.query_noisy_data(samples = 50, noisy_samples = noisy_samples, \n",
    "                                                noisy_params = [2*dataset.w, dataset.b])\n",
    "    no_reg = regressor(data_train_noisy)  \n",
    "    predictions = no_reg.get_predictions(data_test[0]) \n",
    "    print (\"Rmse error of predictions with \" + str(noisy_samples) +  \" samples  = \" + \\\n",
    "            str(rmse(data_test[1], predictions)))\n",
    "    no_reg.plot(data_train_noisy)\n",
    "    display.clear_output(wait=True)\n",
    "    sleep(.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_noisy = dataset.query_noisy_data(samples = 50, noisy_samples = 5, \\\n",
    "                                            noisy_params = [2*dataset.w, dataset.b])\n",
    "for alpha in range (0,5000,100):\n",
    "    l2_reg = ridge_regressor(data_train_noisy, alpha = alpha )\n",
    "    predictions = l2_reg.get_predictions(data_test[0]) \n",
    "    print (\"Rmse error of predictions with \" + str(alpha) +  \" alpha  = \" + \\\n",
    "            str(rmse(data_test[1], predictions)))\n",
    "    l2_reg.plot(data_train_noisy)\n",
    "    display.clear_output(wait=True)\n",
    "    sleep(.1)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
